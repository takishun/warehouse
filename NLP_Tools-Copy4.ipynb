{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 実装予定リスト\n",
    "- ネガポジ値算出\n",
    "- ~~ワードクラウド~~\n",
    "- ~~共起ネットワーク~~\n",
    "    1. ~~単語組み合わせ~~\n",
    "    2. ~~ネットワーク可視化~~\n",
    "    3. ~~英語対応~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import jaconv\n",
    "import itertools\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "from matplotlib import pyplot as plt, font_manager\n",
    "from wordcloud import WordCloud\n",
    "from janome.tokenizer import Tokenizer\n",
    "\n",
    "font_manager._rebuild()\n",
    "if os.name=='nt':\n",
    "    #windows用\n",
    "    font_dir=font_manager.win32FontDirectory()\n",
    "else:\n",
    "    #mac用\n",
    "    font_dir='/Users/pydata/Library/Fonts/'\n",
    "font_path=os.path.join(font_dir,'SourceHanCodeJP-Regular.otf')\n",
    "font=font_manager.FontProperties(fname=font_path,size=14)\n",
    "\n",
    "def preprocess(text,mode=0):   \n",
    "    #mode==1で英語対応、mode==0で日本語対応\n",
    "    if mode==0:\n",
    "        words = ja_tokenize(text)\n",
    "    else:\n",
    "        text = text.lower()\n",
    "        text = text.replace('.',' .')\n",
    "        words = text.split(' ')\n",
    "    \n",
    "    word_to_id={}\n",
    "    id_to_word={}\n",
    "    for word in words:\n",
    "        if word not in word_to_id:\n",
    "            new_id=len(word_to_id)\n",
    "            word_to_id[word]=new_id\n",
    "            id_to_word[new_id]=word\n",
    "    \n",
    "    corpus=np.array([word_to_id[w] for w in words])\n",
    "    return corpus,word_to_id,id_to_word\n",
    "\n",
    "def create_co_matrix(corpus,vocab_size,window_size=1):\n",
    "    corpus_size=len(corpus)\n",
    "    co_matrix=np.zeros((vocab_size, vocab_size),dtype=np.int32)\n",
    "    \n",
    "    for idx, word_id in enumerate(corpus):\n",
    "        for i in range(1,window_size+1):\n",
    "            left_idx=idx-i\n",
    "            right_idx=idx+i\n",
    "            \n",
    "            if left_idx>=0:\n",
    "                left_word_id=corpus[left_idx]\n",
    "                co_matrix[word_id, left_word_id] += 1\n",
    "            \n",
    "            if right_idx<corpus_size:\n",
    "                right_word_id=corpus[right_idx]\n",
    "                co_matrix[word_id,right_word_id] += 1\n",
    "        \n",
    "    return co_matrix\n",
    "\n",
    "def cos_similarity(x,y,eps=1e-8):\n",
    "    nx = x / np.sqrt(np.sum(x**2)+eps)\n",
    "    ny = y / np.sqrt(np.sum(y**2)+eps)\n",
    "    return np.dot(nx,ny)\n",
    "\n",
    "def most_similar(query,word_to_id,id_to_word,word_matrix,top=5):\n",
    "    #1 take query\n",
    "    if query not in word_to_id:\n",
    "        print('%s is not found' %query)\n",
    "        return\n",
    "    \n",
    "    print('\\n[query]'+query)\n",
    "    query_id=word_to_id[query]\n",
    "    query_vec=word_matrix[query_id]\n",
    "    \n",
    "    #2 cal cos similarity\n",
    "    vocab_size=len(id_to_word)\n",
    "    similarity=np.zeros(vocab_size)\n",
    "    for i in range(vocab_size):\n",
    "        similarity[i]=cos_similarity(word_matrix[i],query_vec)\n",
    "    \n",
    "    count=0\n",
    "    for i in (-1*similarity).argsort():\n",
    "        if id_to_word[i] == query:\n",
    "            continue\n",
    "        print('%s: %s' %(id_to_word[i], similarity[i]))\n",
    "        \n",
    "        count+=1\n",
    "        if count >= top:\n",
    "            return\n",
    "\n",
    "def ppmi(C,verbose=False,eps=1e-8):\n",
    "    M=np.zeros_like(C,dtype=np.float32)\n",
    "    N=np.sum(C)\n",
    "    S=np.sum(C,axis=0)\n",
    "    total=C.shape[0]*C.shape[1]\n",
    "    cnt=0\n",
    "    \n",
    "    for i in range(C.shape[0]):\n",
    "        for j in range(C.shape[1]):\n",
    "            pmi=np.log2(C[i,j]*N/(S[j]*S[i])+eps)\n",
    "            M[i,j]=max(0,pmi)\n",
    "            \n",
    "            if verbose:\n",
    "                cnt+=1\n",
    "                if cnt % (total/100)==0:\n",
    "                    print('%.1f%% done' % (100*cnt/total))\n",
    "    return M        \n",
    "\n",
    "def plotting(U,height=5,width=5):\n",
    "    import matplotlib.pyplot as plt\n",
    "    %matplotlib inline\n",
    "    plt.figure(figsize=(height,width))\n",
    "    for word, word_id in word_to_id.items():\n",
    "        plt.annotate(word, (U[word_id, 0],U[word_id,1]))\n",
    "    plt.scatter(U[:,0],U[:,1],alpha=0.5)\n",
    "    plt.show()\n",
    "\n",
    "def ja_tokenize(text):\n",
    "    ja_tokenizer = Tokenizer()\n",
    "    res=[]\n",
    "    stop_words = [u'＇',u\"'\",u'てる', u'いる', u'なる', u'れる', u'する', u'ある', u'こと', u'これ', u'さん', u'して',u'くれる', u'やる', u'くださる', u'そう', u'せる', u'した',  u'思う',u'それ', u'ここ', u'ちゃん', u'くん', u'', u'て',u'に',u'を',u'は',u'の', u'が', u'と', u'た', u'し', u'で',u'ない', u'も', u'な', u'い', u'か', u'ので', u'よう', u'',u'/\\r?\\n/g']\n",
    "    \n",
    "    #remove char\n",
    "    text=re.sub('/','',text)\n",
    "    text=re.sub('セ','',text)\n",
    "    text=re.sub('{}','',text)\n",
    "    text=re.sub('　','',text)\n",
    "    text=re.sub(' ','',text)\n",
    "    text=re.sub(':','',text)\n",
    "    text=jaconv.h2z(text,digit=True, ascii=True)\n",
    "    \n",
    "    lines=text.split('\\n')\n",
    "\n",
    "    for line in lines:\n",
    "        malist = ja_tokenizer.tokenize(line)\n",
    "        for tok in malist:\n",
    "            ps=tok.part_of_speech.split(\",\")[0]\n",
    "            if not ps in ['名詞', '動詞', '形容詞', '形容動詞']: continue\n",
    "            w=tok.base_form\n",
    "            if w==\"*\" or w==\"\": w=tok.surface\n",
    "            if w==\"\" or w==\"\\n\": continue\n",
    "            if w not in stop_words:\n",
    "                res.append(w)\n",
    "        res.append(\"\\n\")\n",
    "    return res\n",
    "\n",
    "def en_tokenize(text):\n",
    "    text=re.sub(',','',text)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    word_tokens = word_tokenize(text)\n",
    "    filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
    "    filtered_sentence = []\n",
    "\n",
    "    for w in word_tokens:\n",
    "        if w not in stop_words:\n",
    "            filtered_sentence.append(w)\n",
    "            \n",
    "    return filtered_sentence\n",
    "    \n",
    "def read_file(filename='cvc_contents.txt'):\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        text=f.read()\n",
    "    text=re.sub('\\n','',text)\n",
    "    return text\n",
    "\n",
    "def convert_one_hot(corpus, vocab_size):\n",
    "    \"\"\"\n",
    "    convert to one hot vector\n",
    "    :param corpus: word id list (1 dimension or 2 dimension NumPy array)\n",
    "    :param vocab_size: vocabulary size\n",
    "    :return: one-hot vector (2 dimension or 3 dimension NumPy array)\n",
    "    \"\"\"\n",
    "    N = corpus.shape[0]\n",
    "\n",
    "    if corpus.ndim == 1:\n",
    "        one_hot = np.zeros((N, vocab_size), dtype=np.int32)\n",
    "\n",
    "        for idx, word_id in enumerate(corpus):\n",
    "            one_hot[idx, word_id] = 1\n",
    "\n",
    "    elif corpus.ndim == 2:\n",
    "        C = corpus.shape[1]\n",
    "        one_hot = np.zeros((N, C, vocab_size), dtype=np.int32)\n",
    "        for idx_0, word_ids in enumerate(corpus):\n",
    "            for idx_1, word_id in enumerate(word_ids):\n",
    "                one_hot[idx_0, idx_1, word_id] = 1\n",
    "\n",
    "    return one_hot\n",
    "\n",
    "def create_contexts_target(corpus,window_size=1):\n",
    "    target=corpus[window_size:-window_size]\n",
    "    contexts=[]\n",
    "    \n",
    "    for idx in range(window_size,len(corpus)-window_size):\n",
    "        cs=[]\n",
    "        for t in range(-window_size, window_size+1):\n",
    "            if t == 0:\n",
    "                continue\n",
    "            cs.append(corpus[idx+t])\n",
    "        contexts.append(cs)\n",
    "    return np.array(contexts), np.array(target)\n",
    "\n",
    "def display(word_dic,width=10,height=10,top=10):\n",
    "    %matplotlib inline\n",
    "    key,value=zip(*word_dic.most_common(top))\n",
    "    plt.figure(figsize=(height,width))\n",
    "    plt.bar(key,value,color='orange')\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.show()\n",
    "    \n",
    "def word_cloud(text,bg_color=\"white\",width=800,height=450):\n",
    "    #文字列除去\n",
    "    text=str(text)\n",
    "    text=re.sub('/\\r\\n/g','',text)\n",
    "    text=re.sub(\"'\",'',text)\n",
    "    \n",
    "    stop_words = [u'＇',u\"'\",u'てる', u'いる', u'なる', u'れる', u'する', u'ある', u'こと', u'これ', u'さん', u'して',u'くれる', u'やる', u'くださる', u'そう', u'せる', u'した',  u'思う',u'それ', u'ここ', u'ちゃん', u'くん', u'', u'て',u'に',u'を',u'は',u'の', u'が', u'と', u'た', u'し', u'で',u'ない', u'も', u'な', u'い', u'か', u'ので', u'よう', u'',u'/\\r?\\n/g']\n",
    "    wordcloud=WordCloud(background_color=bg_color,font_path=\"C:\\Windows\\Fonts\\Yu Gothic UI\\YuGothM.ttc\",width=width,height=height,stopwords=set(stop_words)).generate(text)\n",
    "    plt.figure(figsize=(15,12))\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "#     wordcloud.to_file('./wordcloud.png')\n",
    "\n",
    "def comb_words(text,mode=0):\n",
    "    import pandas as pd\n",
    "    \n",
    "    if mode == 0:\n",
    "        sentences=[ja_tokenize(word) for word in text.split('\\n')]\n",
    "    elif mode == 1:\n",
    "        sentences=[en_tokenaize(word) for word in text.split('.')]\n",
    "    sentence_combinations=[list(itertools.combinations(sentence,2)) for sentence in sentences]\n",
    "    sentence_combinations=[[tuple(sorted(words)) for words in sentence] for sentence in sentence_combinations]\n",
    "    target_combinations=[]\n",
    "    \n",
    "    for sentence in sentence_combinations:\n",
    "        target_combinations.extend(sentence)\n",
    "    \n",
    "    count=Counter(target_combinations)\n",
    "    pd.DataFrame([{'first':i[0][0],'second':i[0][1],'count':i[1]} for i in count.most_common()]).to_csv('kyoki.csv',index=False)\n",
    "    \n",
    "def word_network():\n",
    "    from pyvis.network import Network\n",
    "    import pandas as pd\n",
    "    \n",
    "    got_net=Network(height=\"1000px\", width=\"95%\",bgcolor=\"#FFFFFF\",font_color=\"black\",notebook=True)\n",
    "    got_net.barnes_hut()\n",
    "    got_data=pd.read_csv(\"kyoki.csv\")[:150]\n",
    "    \n",
    "    sources=got_data['first']#count\n",
    "    targets=got_data['second']#first\n",
    "    weights=got_data['count']#second\n",
    "    \n",
    "    edge_data=zip(sources, targets, weights)\n",
    "    \n",
    "    for e in edge_data:\n",
    "        src=e[0]\n",
    "        dst=e[1]\n",
    "        w=e[2]\n",
    "        \n",
    "        got_net.add_node(src,src,title=src)\n",
    "        got_net.add_node(dst,dst,title=dst)\n",
    "        got_net.add_edge(src,dst,value=w)\n",
    "    \n",
    "    neighbor_map=got_net.get_adj_list()\n",
    "    \n",
    "    for node in got_net.nodes:\n",
    "        node[\"title\"]+=\" Neighbors:<br>\"+\"<br>\".join(neighbor_map[node['id']])\n",
    "        node[\"value\"]=len(neighbor_map[node['id']])\n",
    "    \n",
    "    got_net.show_buttons(filter_=['physics'])\n",
    "    got_net.show('output.html')\n",
    "    \n",
    "def similarity(word1,word2,word_to_id):\n",
    "    if word1 not in word_to_id:\n",
    "        print('%s is not found' %word1)\n",
    "        return\n",
    "    \n",
    "    if word2 not in word_to_id:\n",
    "        print('%s is not found' %word2)\n",
    "        return\n",
    "    \n",
    "    return cos_similarity(C[word_to_id[word1]],C[word_to_id[word2]])\n",
    "    \n",
    "def negaposi(text):\n",
    "    pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ ==\"__main__\":    \n",
    "    start = time.time()\n",
    "    text=\"Artificial sweeteners can make you feel more hungry, despite their calorie-free or low-calorie content, research finds. They have an appetite-stimulating effect on the brain, causing people to eat more and actually increasing the risk of putting on weight.\"\n",
    "    text2=\"aa\"  \n",
    "    text3=read_file()\n",
    "#     word_cloud(en_tokenize(text))\n",
    "#     print(en_tokenize(text))\n",
    "    comb_words(text2,0)\n",
    "    \n",
    "    word_cloud(ja_tokenize(text2),bg_color='white')\n",
    "    word_network()\n",
    "\n",
    "#     word_count = Counter(en_tokenize(text))\n",
    "    word_count = Counter(ja_tokenize(text2))\n",
    "#     print(word_count.most_common(20))\n",
    "    display(word_count,top=20)\n",
    "#     print('elapsed time word_count: ',time.time()-start)\n",
    "\n",
    "    window_size=2\n",
    "    wordvec_size=100\n",
    "    corpus,word_to_id,id_to_word=preprocess(text2,0)\n",
    "    print('elapsed time preprocess point: ',time.time()-start)\n",
    "    vocab_size=len(word_to_id)\n",
    "    print('counting co-occurrence...')\n",
    "    C=create_co_matrix(corpus,vocab_size,window_size)\n",
    "    print('elapsed time create co-matrix point: ',time.time()-start)\n",
    "    print('caliculating PPMI...')\n",
    "    W=ppmi(C,verbose=True)\n",
    "    print('elapsed time point PPMI: ',time.time()-start)\n",
    "    print('calizulating SVD...')\n",
    "    \n",
    "    try:\n",
    "        from sklearn.utils.extmath import randomized_svd\n",
    "        U,S,V=randomized_svd(W,n_components=wordvec_size,n_iter=5,random_state=None)\n",
    "    except ImportError:\n",
    "        U,S,V=np.linalg.svd(W)\n",
    "        \n",
    "    print('elapsed time point SVD: ',time.time()-start)\n",
    "    \n",
    "    word_vecs=U[:,:wordvec_size]\n",
    "    querys=['エクリプスクロス','コルト','アウトランダー','三菱']\n",
    "    plotting(U,10,10)\n",
    "    for query in querys:\n",
    "        most_similar(query,word_to_id,id_to_word,word_vecs,top=5)\n",
    "    \n",
    "\n",
    "#     print('total elapsed time: ',time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
