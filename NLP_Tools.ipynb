{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 実装リスト\n",
    "- ネガポジ値算出\n",
    "- ~~ワードクラウド~~\n",
    "- ~~共起ネットワーク~~\n",
    "    1. ~~単語組み合わせ~~\n",
    "    2. ~~ネットワーク可視化~~\n",
    "    3. ~~英語対応~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import jaconv\n",
    "import itertools\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "from matplotlib import pyplot as plt, font_manager\n",
    "from wordcloud import WordCloud\n",
    "from janome.tokenizer import Tokenizer\n",
    "\n",
    "font_manager._rebuild()\n",
    "if os.name=='nt':\n",
    "    #windows用\n",
    "    font_dir=font_manager.win32FontDirectory()\n",
    "else:\n",
    "    #mac用\n",
    "    font_dir='/Users/pydata/Library/Fonts/'\n",
    "font_path=os.path.join(font_dir,'SourceHanCodeJP-Regular.otf')\n",
    "font=font_manager.FontProperties(fname=font_path,size=14)\n",
    "\n",
    "def preprocess(text,mode=0):   \n",
    "    #mode==1で英語対応、mode==0で日本語対応\n",
    "    if mode==1:\n",
    "        words = ja_tokenize(text)\n",
    "    else:\n",
    "        text = text.lower()\n",
    "        text = text.replace('.',' .')\n",
    "        words = text.split(' ')\n",
    "    \n",
    "    word_to_id={}\n",
    "    id_to_word={}\n",
    "    for word in words:\n",
    "        if word not in word_to_id:\n",
    "            new_id=len(word_to_id)\n",
    "            word_to_id[word]=new_id\n",
    "            id_to_word[new_id]=word\n",
    "    \n",
    "    corpus=np.array([word_to_id[w] for w in words])\n",
    "    return corpus,word_to_id,id_to_word\n",
    "\n",
    "def create_co_matrix(corpus,vocab_size,window_size=1):\n",
    "    corpus_size=len(corpus)\n",
    "    co_matrix=np.zeros((vocab_size, vocab_size),dtype=np.int32)\n",
    "    \n",
    "    for idx, word_id in enumerate(corpus):\n",
    "        for i in range(1,window_size+1):\n",
    "            left_idx=idx-i\n",
    "            right_idx=idx+i\n",
    "            \n",
    "            if left_idx>=0:\n",
    "                left_word_id=corpus[left_idx]\n",
    "                co_matrix[word_id, left_word_id] += 1\n",
    "            \n",
    "            if right_idx<corpus_size:\n",
    "                right_word_id=corpus[right_idx]\n",
    "                co_matrix[word_id,right_word_id] += 1\n",
    "        \n",
    "    return co_matrix\n",
    "\n",
    "def cos_similarity(x,y,eps=1e-8):\n",
    "    nx = x / np.sqrt(np.sum(x**2)+eps)\n",
    "    ny = y / np.sqrt(np.sum(y**2)+eps)\n",
    "    return np.dot(nx,ny)\n",
    "\n",
    "def most_similar(query,word_to_id,id_to_word,word_matrix,top=5):\n",
    "    #1 take query\n",
    "    if query not in word_to_id:\n",
    "        print('%s is not found' %query)\n",
    "        return\n",
    "    \n",
    "    print('\\n[query]'+query)\n",
    "    query_id=word_to_id[query]\n",
    "    query_vec=word_matrix[query_id]\n",
    "    \n",
    "    #2 cal cos similarity\n",
    "    vocab_size=len(id_to_word)\n",
    "    similarity=np.zeros(vocab_size)\n",
    "    for i in range(vocab_size):\n",
    "        similarity[i]=cos_similarity(word_matrix[i],query_vec)\n",
    "    \n",
    "    count=0\n",
    "    for i in (-1*similarity).argsort():\n",
    "        if id_to_word[i] == query:\n",
    "            continue\n",
    "        print('%s: %s' %(id_to_word[i], similarity[i]))\n",
    "        \n",
    "        count+=1\n",
    "        if count >= top:\n",
    "            return\n",
    "\n",
    "def ppmi(C,verbose=False,eps=1e-8):\n",
    "    M=np.zeros_like(C,dtype=np.float32)\n",
    "    N=np.sum(C)\n",
    "    S=np.sum(C,axis=0)\n",
    "    total=C.shape[0]*C.shape[1]\n",
    "    cnt=0\n",
    "    \n",
    "    for i in range(C.shape[0]):\n",
    "        for j in range(C.shape[1]):\n",
    "            pmi=np.log2(C[i,j]*N/(S[j]*S[i])+eps)\n",
    "            M[i,j]=max(0,pmi)\n",
    "            \n",
    "            if verbose:\n",
    "                cnt+=1\n",
    "                if cnt % (total/100)==0:\n",
    "                    print('%.1f%% done' % (100*cnt/total))\n",
    "    return M        \n",
    "\n",
    "def plotting(U,height=5,width=5):\n",
    "    import matplotlib.pyplot as plt\n",
    "    %matplotlib inline\n",
    "    plt.figure(figsize=(height,width))\n",
    "    for word, word_id in word_to_id.items():\n",
    "        plt.annotate(word, (U[word_id, 0],U[word_id,1]))\n",
    "    plt.scatter(U[:,0],U[:,1],alpha=0.5)\n",
    "    plt.show()\n",
    "\n",
    "def ja_tokenize(text):\n",
    "    ja_tokenizer = Tokenizer()\n",
    "    res=[]\n",
    "    \n",
    "    #stop word\n",
    "    text=re.sub('/','',text)\n",
    "    text=re.sub('セ','',text)\n",
    "    text=re.sub('{}','',text)\n",
    "    text=re.sub('　','',text)\n",
    "    text=re.sub(' ','',text)\n",
    "    text=re.sub(':','',text)\n",
    "    \n",
    "    text=jaconv.h2z(text,digit=True, ascii=True)\n",
    "    \n",
    "    lines=text.split('\\n')\n",
    "\n",
    "    for line in lines:\n",
    "        malist = ja_tokenizer.tokenize(line)\n",
    "        for tok in malist:\n",
    "            ps=tok.part_of_speech.split(\",\")[0]\n",
    "            if not ps in ['名詞', '動詞', '形容詞', '形容動詞']: continue\n",
    "            w=tok.base_form\n",
    "            if w==\"*\" or w==\"\": w=tok.surface\n",
    "            if w==\"\" or w==\"\\n\": continue\n",
    "            res.append(w)\n",
    "        res.append(\"\\n\")\n",
    "    return res\n",
    "\n",
    "def en_tokenize(text):\n",
    "    text=re.sub(',','',text)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    word_tokens = word_tokenize(text)\n",
    "    filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
    "    filtered_sentence = []\n",
    "\n",
    "    for w in word_tokens:\n",
    "        if w not in stop_words:\n",
    "            filtered_sentence.append(w)\n",
    "            \n",
    "#     print(word_tokens)\n",
    "#     print(filtered_sentence)\n",
    "    return filtered_sentence\n",
    "    \n",
    "def read_file(filename='cvc_contents.txt'):\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        text=f.read()\n",
    "    text=re.sub('\\n','',text)\n",
    "    return text\n",
    "\n",
    "def convert_one_hot(corpus, vocab_size):\n",
    "    \"\"\"\n",
    "    convert to one hot vector\n",
    "    :param corpus: word id list (1 dimension or 2 dimension NumPy array)\n",
    "    :param vocab_size: vocabulary size\n",
    "    :return: one-hot vector (2 dimension or 3 dimension NumPy array)\n",
    "    \"\"\"\n",
    "    N = corpus.shape[0]\n",
    "\n",
    "    if corpus.ndim == 1:\n",
    "        one_hot = np.zeros((N, vocab_size), dtype=np.int32)\n",
    "\n",
    "        for idx, word_id in enumerate(corpus):\n",
    "            one_hot[idx, word_id] = 1\n",
    "\n",
    "    elif corpus.ndim == 2:\n",
    "        C = corpus.shape[1]\n",
    "        one_hot = np.zeros((N, C, vocab_size), dtype=np.int32)\n",
    "        for idx_0, word_ids in enumerate(corpus):\n",
    "            for idx_1, word_id in enumerate(word_ids):\n",
    "                one_hot[idx_0, idx_1, word_id] = 1\n",
    "\n",
    "    return one_hot\n",
    "\n",
    "def create_contexts_target(corpus,window_size=1):\n",
    "    target=corpus[window_size:-window_size]\n",
    "    contexts=[]\n",
    "    \n",
    "    for idx in range(window_size,len(corpus)-window_size):\n",
    "        cs=[]\n",
    "        for t in range(-window_size, window_size+1):\n",
    "            if t == 0:\n",
    "                continue\n",
    "            cs.append(corpus[idx+t])\n",
    "        contexts.append(cs)\n",
    "    return np.array(contexts), np.array(target)\n",
    "\n",
    "def keyword_check(part):\n",
    "    return re.match('名詞' or '動詞' or '形容詞' or'形容動詞',part)\n",
    "\n",
    "def text_analyze(text):\n",
    "    t=Tokenizer()\n",
    "    tokens=t.tokenize(text)\n",
    "    result=[]\n",
    "    \n",
    "    for token in tokens:\n",
    "        result.append([token.surface,token.part_of_speech])\n",
    "    return result\n",
    "\n",
    "def make_freq(text,encoding='utf-8'):\n",
    "#     with open(file,'r',encoding='utf-8') as f:\n",
    "#         text=f.read()\n",
    "    \n",
    "    word_dic={}\n",
    "    analyze_list=text_analyze(text)\n",
    "    \n",
    "    for word, part in analyze_list:\n",
    "        if(keyword_check(part)):\n",
    "            if word in word_dic:\n",
    "                word_dic[word]+=1\n",
    "            else:\n",
    "                word_dic[word]=1\n",
    "                \n",
    "    return(word_dic)\n",
    "\n",
    "def display(word_dic,width=10,height=10,top=10):\n",
    "    %matplotlib inline\n",
    "    key,value=zip(*word_dic.most_common(top))\n",
    "    plt.figure(figsize=(height,width))\n",
    "    plt.bar(key,value,color='orange')\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.show()\n",
    "    \n",
    "def word_cloud(text,bg_color=\"white\",width=800,height=450):\n",
    "    #文字列除去\n",
    "    text=str(text)\n",
    "    text=re.sub('/\\r\\n/g','',text)\n",
    "    text=re.sub(\"'\",'',text)\n",
    "    \n",
    "    stop_words = [u'＇',u\"'\",u'てる', u'いる', u'なる', u'れる', u'する', u'ある', u'こと', u'これ', u'さん', u'して',u'くれる', u'やる', u'くださる', u'そう', u'せる', u'した',  u'思う',u'それ', u'ここ', u'ちゃん', u'くん', u'', u'て',u'に',u'を',u'は',u'の', u'が', u'と', u'た', u'し', u'で',u'ない', u'も', u'な', u'い', u'か', u'ので', u'よう', u'',u'/\\r?\\n/g']\n",
    "    wordcloud=WordCloud(background_color=bg_color,font_path=\"C:\\Windows\\Fonts\\Yu Gothic UI\\YuGothM.ttc\",width=width,height=height,stopwords=set(stop_words)).generate(text)\n",
    "    plt.figure(figsize=(15,12))\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "#     wordcloud.to_file('./wordcloud.png')\n",
    "\n",
    "def comb_words(text,mode=0):\n",
    "    import pandas as pd\n",
    "    \n",
    "    if mode == 0:\n",
    "        sentences=[ja_tokenize(word) for word in text.split('\\n')]\n",
    "    elif mode == 1:\n",
    "        sentences=[en_tokenaize(word) for word in text.split('.')]\n",
    "    sentence_combinations=[list(itertools.combinations(sentence,2)) for sentence in sentences]\n",
    "    sentence_combinations=[[tuple(sorted(words)) for words in sentence] for sentence in sentence_combinations]\n",
    "    target_combinations=[]\n",
    "    \n",
    "    for sentence in sentence_combinations:\n",
    "        target_combinations.extend(sentence)\n",
    "    \n",
    "    count=Counter(target_combinations)\n",
    "    pd.DataFrame([{'first':i[0][0],'second':i[0][1],'count':i[1]} for i in count.most_common()]).to_csv('kyoki.csv',index=False)\n",
    "#     print(count.most_common()[:10])\n",
    "    \n",
    "def word_network():\n",
    "    from pyvis.network import Network\n",
    "    import pandas as pd\n",
    "    \n",
    "    got_net=Network(height=\"1000px\", width=\"95%\",bgcolor=\"#FFFFFF\",font_color=\"black\",notebook=True)\n",
    "    got_net.barnes_hut()\n",
    "#     got_net.force_atlas_2based()\n",
    "    got_data=pd.read_csv(\"kyoki.csv\")[:150]\n",
    "    \n",
    "    sources=got_data['first']#count\n",
    "    targets=got_data['second']#first\n",
    "    weights=got_data['count']#second\n",
    "    \n",
    "    edge_data=zip(sources, targets, weights)\n",
    "    \n",
    "    for e in edge_data:\n",
    "        src=e[0]\n",
    "        dst=e[1]\n",
    "        w=e[2]\n",
    "        \n",
    "        got_net.add_node(src,src,title=src)\n",
    "        got_net.add_node(dst,dst,title=dst)\n",
    "        got_net.add_edge(src,dst,value=w)\n",
    "    \n",
    "    neighbor_map=got_net.get_adj_list()\n",
    "    \n",
    "    for node in got_net.nodes:\n",
    "        node[\"title\"]+=\"Neighbors:<br>\"+\"<br>\".join(neighbor_map[node['id']])\n",
    "        node[\"value\"]=len(neighbor_map[node['id']])\n",
    "    \n",
    "    got_net.show_buttons(filter_=['physics'])\n",
    "    got_net.show('output.html')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-f896044d6fd8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m#     word_cloud(en_tokenize(text))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m#     print(en_tokenize(text))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0mcomb_words\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m     \u001b[0mword_network\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-26-a9990b7222b9>\u001b[0m in \u001b[0;36mcomb_words\u001b[1;34m(text, mode)\u001b[0m\n\u001b[0;32m    264\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    265\u001b[0m         \u001b[0msentences\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0men_tokenaize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 266\u001b[1;33m     \u001b[0msentence_combinations\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitertools\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcombinations\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msentences\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    267\u001b[0m     \u001b[0msentence_combinations\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mwords\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msentence\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msentence_combinations\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    268\u001b[0m     \u001b[0mtarget_combinations\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-26-a9990b7222b9>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    264\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    265\u001b[0m         \u001b[0msentences\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0men_tokenaize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 266\u001b[1;33m     \u001b[0msentence_combinations\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitertools\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcombinations\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msentences\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    267\u001b[0m     \u001b[0msentence_combinations\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mwords\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msentence\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msentence_combinations\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    268\u001b[0m     \u001b[0mtarget_combinations\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ ==\"__main__\":\n",
    "    \n",
    "#     start = time.time()\n",
    "#     text=\"Artificial sweeteners can make you feel more hungry, despite their calorie-free or low-calorie content, research finds. They have an appetite-stimulating effect on the brain, causing people to eat more and actually increasing the risk of putting on weight.\"\n",
    "#     text2=\"三菱自動車工業株式会社(本社：東京都港区、代表執行役CEO：加藤 隆雄、以下三菱自動車)は、第46回東京モーターショー2019*1においてプレスカンファレンスを実施し、スモールサイズの電動SUVコンセプトカー『MI-TECH CONCEPT（マイテックコンセプト）*2』および軽コンセプトカー『SUPER HEIGHT K-WAGON CONCEPT（スーパーハイト軽ワゴンコンセプト）』を世界初披露しました。今回のプレスカンファレンスではCEO加藤隆雄およびCOOグプタ・アシュワニがスピーチし、電動車戦略については「当社は電動化技術、特にPHEVを得意としており、これからもPHEVカテゴリーをリードすべく、さらなるバリエーションの拡充を推進するとともに、アライアンスにおける多様な電動化技術を活用することで、電動車のラインナップを拡充します。具体的には2022年までにミッドサイズSUV、コンパクトSUVに、いずれかの電動化技術を採用した新型車を投入する計画です。今後軽自動車を含め電動化を進めていきます」と述べました。また、軽自動車については「今回提案する『SUPER HEIGHT K-WAGON CONCEPT』は、激戦区であるスーパーハイトワゴン市場に、新世代軽自動車の第2弾の位置付けとして、本年度内の発売を計画しています」と述べました。\"  \n",
    "    text3=read_file()\n",
    "#     word_cloud(en_tokenize(text))\n",
    "#     print(en_tokenize(text))\n",
    "    comb_words(text3)\n",
    "    word_network()\n",
    "\n",
    "#     word_count = Counter(en_tokenize(text))\n",
    "    word_count = Counter(ja_tokenize(text2))\n",
    "    print(word_count.most_common(20))\n",
    "    display(word_count,top=20)\n",
    "#     print('elapsed time word_count: ',time.time()-start)\n",
    "\n",
    "#     window_size=2\n",
    "#     wordvec_size=100\n",
    "#     corpus,word_to_id,id_to_word=preprocess(text3,1)\n",
    "#     print('elapsed time preprocess point: ',time.time()-start)\n",
    "#     vocab_size=len(word_to_id)\n",
    "#     print('counting co-occurrence...')\n",
    "#     C=create_co_matrix(corpus,vocab_size,window_size)\n",
    "#     print('elapsed time create co-matrix point: ',time.time()-start)\n",
    "#     print('caliculating PPMI...')\n",
    "#     W=ppmi(C,verbose=True)\n",
    "#     print('elapsed time point PPMI: ',time.time()-start)\n",
    "#     print('calizulating SVD...')\n",
    "#     try:\n",
    "#         from sklearn.utils.extmath import randomized_svd\n",
    "#         U,S,V=randomized_svd(W,n_components=wordvec_size,n_iter=5,random_state=None)\n",
    "#     except ImportError:\n",
    "#         U,S,V=np.linalg.svd(W)\n",
    "#     print('elapsed time point SVD: ',time.time()-start)\n",
    "#     word_vecs=U[:,:wordvec_size]\n",
    "#     querys=['エクリプスクロス','コルト','アウトランダー','三菱']\n",
    "#     for query in querys:\n",
    "#         most_similar(query,word_to_id,id_to_word,word_vecs,top=5)\n",
    "#     plotting(U,10,10)\n",
    "#     print('total elapsed time: ',time.time()-start)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.38775264490870526\n"
     ]
    }
   ],
   "source": [
    "c0=C[word_to_id['三菱']]\n",
    "c1=C[word_to_id['コルト']]\n",
    "print(cos_similarity(c0,c1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[query]トヨタ\n",
      "以前: 0.7216825614592476\n",
      "の: 0.6995351319288472\n",
      "試乗: 0.6980730342424565\n",
      "自分: 0.6969989352767775\n",
      "今: 0.6969200774203907\n"
     ]
    }
   ],
   "source": [
    "most_similar('トヨタ',word_to_id,id_to_word, C, top=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:98: RuntimeWarning: overflow encountered in long_scalars\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:98: RuntimeWarning: invalid value encountered in log2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "covariance matrix\n",
      "[[2 1 0 ... 0 0 0]\n",
      " [1 0 1 ... 0 0 1]\n",
      " [0 1 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 1 0 ... 0 0 0]]\n",
      "--------------------------------------------------\n",
      "PPMI\n",
      "[[ 0.     6.318  0.    ...  0.     0.     0.   ]\n",
      " [ 6.318  0.     7.154 ...  0.     0.    17.932]\n",
      " [ 0.     7.154  0.    ...  0.     0.     0.   ]\n",
      " ...\n",
      " [ 0.     0.     0.    ...  0.     0.     0.   ]\n",
      " [ 0.     0.     0.    ...  0.     0.     0.   ]\n",
      " [ 0.    17.932  0.    ...  0.     0.     0.   ]]\n"
     ]
    }
   ],
   "source": [
    "sys.path.append('..')\n",
    "\n",
    "# text='これはぺんですか？いいえ、ベンです。ケフィアかもしれません。'\n",
    "corpus,word_to_id,id_to_word=preprocess(text3,1)\n",
    "vocab_size=len(word_to_id)\n",
    "C=create_co_matrix(corpus, vocab_size)\n",
    "W=ppmi(C)\n",
    "\n",
    "np.set_printoptions(precision=3)\n",
    "print('covariance matrix')\n",
    "# print('text:'+ text)\n",
    "print(C)\n",
    "print('-'*50)\n",
    "print('PPMI')\n",
    "print(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 1 5 6]\n",
      "{0: 'you', 1: 'say', 2: 'goodbye', 3: 'and', 4: 'i', 5: 'hello', 6: '.'}\n"
     ]
    }
   ],
   "source": [
    "text = 'you say goodbye and I say hello.'\n",
    "corpus, word_to_id,id_to_word=preprocess(text)\n",
    "print(corpus)\n",
    "print(id_to_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus,word_to_id,id_to_word=preprocess(text)\n",
    "context, target=create_contexts_target(corpus,window_size=1)\n",
    "vocab_size=len(word_to_id)\n",
    "target=convert_one_hot(target,vocab_size)\n",
    "contexts=convert_one_hot(contexts,vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size=1\n",
    "hidden_size=5\n",
    "batch_size=3\n",
    "max_epoch=1000\n",
    "\n",
    "text='You say goodbye and I say hello.'\n",
    "corpus,word_to_id,id_to_word=preprocess(text)\n",
    "\n",
    "vocab_size=len(word_to_id)\n",
    "contexts,target=create_contexts_target(corpus,window_size)\n",
    "target=convert_one_hot(target,vocab_size)\n",
    "contexts=convert_one_hot(contexts,vocab_size)\n",
    "\n",
    "model=SimpleCBOW(vocab_size,hidden_size)\n",
    "optimizer=Adam()\n",
    "trainer=Trainer(model,optimizer)\n",
    "\n",
    "trainer.fit(contexts,target,max_epoch,batch_size)\n",
    "trainer.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'your', 'theirs', \"you'd\", 'further', 'ours', 'over', 'can', 'under', 'our', 'my', 'he', 'what', 'll', 'more', 'and', 'all', 'did', 'as', 'them', 'above', \"should've\", 'do', 'haven', 'so', 'me', 'shouldn', 'you', 'it', 'which', 's', 'y', \"shouldn't\", \"wouldn't\", 'between', 'then', 've', 'needn', 'once', 'hers', 'these', 'why', 'mustn', 'am', \"haven't\", 'only', 'but', 'both', 'wasn', 'being', 'very', 'through', \"that'll\", 'be', 'don', 're', 'by', 'here', 'himself', 'm', 'at', 'same', 'out', \"isn't\", 'does', 'she', 'been', 'during', 'or', 'such', 'about', 'below', \"mustn't\", 'should', 'for', \"mightn't\", \"needn't\", 'i', 'yours', 'him', 'mightn', 'an', 'doesn', 'myself', 'with', 'were', 'when', 't', 'will', 'a', \"hadn't\", \"won't\", 'most', 'yourself', 'off', 'have', \"you've\", \"wasn't\", 'doing', 'few', 'his', 'now', \"it's\", \"you'll\", 'shan', 'on', 'd', 'is', 'those', 'own', 'the', 'are', 'won', 'wouldn', 'ain', \"didn't\", \"weren't\", 'yourselves', 'has', 'this', 'was', 'itself', 'into', 'each', \"hasn't\", 'how', 'again', 'didn', 'that', 'to', 'themselves', 'after', 'down', 'no', 'any', 'in', 'weren', 'some', 'other', 'aren', 'too', 'until', 'o', \"don't\", 'from', 'her', 'who', 'there', \"couldn't\", 'than', 'ourselves', \"you're\", 'their', 'up', 'because', 'had', 'before', 'herself', 'we', 'ma', 'where', \"aren't\", 'isn', 'if', 'not', 'hasn', 'its', 'against', 'just', \"shan't\", 'nor', 'whom', 'hadn', 'of', 'while', 'they', 'having', \"doesn't\", 'couldn', \"she's\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
